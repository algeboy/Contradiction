
\documentclass[12pt,twoside,dvipsnames,letterpaper]{memoir}
\input{tex-preambles/Content.tex}
\input{tex-preambles/tikz-sets.tex}
\input{tex-preambles/math-preambles.tex}
\input{tex-preambles/listings-preamble.tex}

\usepackage{CJKutf8}
\author{James B. Wilson}
\date{\today}
\begin{document}

\chapter{The Four Spaces}


\begin{table}[!htbp]
    \centering
    \includegraphics{4-space-table.pdf}
    \caption{The 4 spaces of a linear map.}
    \label{tab:4-spaces}
\end{table}
\begin{CJK*}{UTF8}{gbsn}
\subsection{ 方程 ~Fang cheng}
\end{CJK*}
Algorithms appearing in China around 2000 years
ago depict the use of linear algebra on matrices 
to solve problems.  The Chinese called these matrices 
\emph{F\u{a}ng} \emph{ch\'eng}.
A thousand years later in Iraq and Iran where inventing algebra
and using it to solve systems of linear equations.
In the late 1600's Leibniz recorded matrix ideas for European 
scholars and may have even been aware of the Chinese Fang cheng
before doing so.  
A number of textbooks today call this process
``Gaussian Elimination'' after the 19th century math polymath 
Carl Friedrich Gauss who systemized the mechanics.  
Now that the Chinese origins are better known the best 
practice in history would promote this attribution instead.

\begin{definition}
    For a natural number $m$, $[m]=\{1,\ldots,m\}$ with $[0]$
    the empty set.
    An \emph{$m\times n$-matrix} $\Phi$ over $\Delta$
    is a function $\Phi:[m]\times [n]\to \Delta$.
    These are also denoted $\Phi\in \Delta^{m\times n}$.
    Evaluating that function as $(i,j)$ is denoted 
    $\Phi_{ij}$.  The coordinates $i$ are called \emph{rows}
    and the $j$'s are colled \emph{columns}.
\end{definition}

Of the many important classes of $(m\times n)$-matrices the following list are the fundamental ones.
\begin{gather}
    \tag{$E_{ij}$}
    (E_{ij})_{kl}  = \left\{\begin{array}{cc}
        1 & i=k, j=l\\
        0 & \text{else}
    \end{array}\right.\\
\end{gather}
These are sometimes called \emph{matrix units}.
For example for $m=3,n=4$ we have
\[
    E_{23}=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}
    \qquad
    E_{22}=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}
    \qquad 
    E_{32}
    =\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}
\]
\begin{proposition}
    Every $(m\times n)$-matrix $\Phi$ is the sum of scaled matrix units,
    \[
        \Phi = \sum_{i}\sum_j \Phi_{ij}E_{ij}.
    \]
\end{proposition}

For $m=n$ we have the following essential matrices.
\begin{gather}
    \tag{$I_n$}
    (I_n)_{ij} = \left\{\begin{array}{cc}
        1 & i=j\\
        0 & \text{else}
    \end{array}\right.\\
    \tag{Transvection}
    I_n+\alpha E_{ij}  \quad i\neq j\\
    \tag{Diagonal}
    Diag(a_1,\ldots,a_n)_{ij} = \left\{\begin{array}{cc}
        a_i & i=j\\
        0 & \text{else}
    \end{array}\right.\\
    \tag{Permutation}
    \Sigma_{ij}\neq 0  \Rightarrow (\Sigma_{ij}=1) \& (k\neq i\Rightarrow \Sigma_{kj}=0)
    \& (j\neq k\Rightarrow \Sigma_{ik}=0).
\end{gather}
These last three families are known as \emph{elementary matrices}.
Each is invertible.

\subsection{Clearing a row}

\begin{definition}
    Fix a ring $\Delta$.
    A coordinate vector $v\in \Delta^m$ is \emph{pivotable}
    if there is a $u\in \Delta^m$ such that 
    \begin{align*}
        u\cdot v & = \sum_{i:[m]} u_i v_i  = 1.
    \end{align*}
\end{definition}

\begin{proposition}
    If $v$ is pivotable and $u\cdot v=1$ then $u$ is also pivotable.
\end{proposition}

In some situations it makes sense to replace the dot-product 
with another bilinear form, for example using complex conjugation
or weighted dot-products or other geometric considerations.

\begin{proposition}
    If $\Delta$ is a division ring then very non-zero vector is pivotable.    
\end{proposition}
\begin{proof}
    if $v\neq 0$ then for some $i$, $v_i\neq 0$.  As $\Delta$ is a division 
    ring, $v_i^{-1}$ exists.  So set $u=v_i^{-1} e_i$. Thus, 
    $u\cdot v=v_i^{-1} v_i=1$.
\end{proof}

\begin{example}
    Over $\mathbb{Z}$, $v=(2,3,7)$ is pivotable but $w=(2,6,14)$ is not.
\end{example}
\begin{proof}
    $1=\gcd(2,3,7)=3\cdot 2+(-1)3+0\cdot 7$ whereas 
    $2a+6b+14c$ is always even and therefore never equal to $1$.
\end{proof}

\begin{proposition}
    Given a matrix $\Phi$ and a pivotable column $j$, 
    then there is an invertible matrix $R_j$ such that $(R_j\Phi)_{*j}=e_i$.
\end{proposition}
\begin{proof}
    Let $R_j$ be the identity $(m\times m)$-matrix with the $i$-th row removed 
    and replaced by some $u$ for which $u\cdot \Phi_{*j}=1$.
    Then 
    \begin{align*}
        (R_j \Phi)_{ij} & = u\cdot \Phi_{*j}=1.
    \end{align*}

    For any other non-zero entry $\Phi_{kj}$, define the transvection 
    \begin{align*}
        T_k & = I_m + (-\Phi_{kj}\Phi_{ij})E_{ik}
    \end{align*}
    It follows that 
    \begin{align*}
        (T_k \Phi)_{kj} = 0.
    \end{align*}
    So it is possible through a sequence of at most $m-1$ transvections 
    to clear the non-zero values in column $j$.
\end{proof}


Given a matrix $\Phi$ and an entry $\Phi_{ij}$ which divides 
all other entries $\Phi_{kj}$,

Augment the matrix $\Phi$ by $I_m$ to create a tableaux
$T=[I_m | \Phi]$.  
The \emph{admissible} rows are initially $\{1,\ldots,m\}$.
Choose an admissible row $i$.  Scan the $\Phi$ block for 
a column $j$ with $\Phi_{ij}\neq 0$. Since we 
are over a division ring $\Phi_{ij}$ is therefore invertible.


Let us consider a matrix in which a subset of the columns 
are an identity matrix.  That is, up to possibly permuting 
the columns the matrix has the form
\begin{align*}
    \begin{bmatrix}
        I_r & M \\ 
        0 & 0 
    \end{bmatrix}\in \Delta^{m\times n}
\end{align*}
Then an answer would be written down with formula 
requiring no computation:
\begin{align*}
    \begin{bmatrix}
        I_r & M \\ 
        0 & 0 
    \end{bmatrix}
    \begin{bmatrix}
        -M  \\ 
        I_{n-r} 
    \end{bmatrix}
    & = 
    \begin{bmatrix}
        0\\
        0
    \end{bmatrix}
\end{align*}


\begin{proposition}
    For every $(m\times n)$-matrix $\Phi$ over a division ring $\Delta$,
    there is an invertible matrix 
    $R$ and a permutation matrix $\Sigma$ such that 
    \begin{align*}
        R\Phi \Sigma & = \begin{bmatrix}
            I_r & M \\
            0 & 0 
        \end{bmatrix}.
    \end{align*}
    Furthermore 
    \begin{align*}
        \Sigma \begin{bmatrix} -M\\ I_{n-r}\end{bmatrix}
    \end{align*}
    is a kernel map $\iota:\Delta^{n-r}\to \Delta^n$.
\end{proposition}
\begin{proof}
    Augment the matrix $\Phi$ by $I_m$ to create a tableaux
    $T=[I_m | \Phi]$.  
    The \emph{admissible} rows are initially $\{1,\ldots,m\}$.
    Choose an admissible row $i$.  Scan the $\Phi$ block for 
    a column $j$ with $\Phi_{ij}\neq 0$. Since we 
    are over a division ring $\Phi_{ij}$ is therefore invertible.


    Select a non-zero column $j$ in the $\Phi$ 
    block that is non-zero.  
    \begin{align*}
        \begin{bmatrix}
            \Phi_{ij}^{-1} I_n 
        \end{bmatrix}
    \end{align*}    
\end{proof}
\begin{lstlisting}[language=Sava]
def enteringCol():Fin[n]

\end{lstlisting}

Suppose 
\begin{align*}
    \Phi & = 
    \begin{bmatrix}
        1 & 0 & -2 & 1\\
        0 & 1 & 2 & 2\\
        0 & 0 & 0 & 0
    \end{bmatrix}
\end{align*}


\begin{definition}
    A matrix $\Phi$ is in \emph{Reduce Row Echelon Form (RREF)}
    if there is a permutation matrix $\Sigma$ of the columns such 
    that 
    \begin{align*}
        \Phi\Sigma & = \begin{bmatrix}
            I_r & M \\
            0 & 0 
        \end{bmatrix}.
    \end{align*}
\end{definition}




\section{Kernels}

\subsection{Kernels in Set language}
Given $\Omega$-modules $V$ and $W$ and a linear map $\varphi:V\to W$, 
\[
    \ker \varphi=\{v\in V\mid \varphi(v)=0\}.
\]
This definition is concise because it's meaning
is outsourced to the concept of a set. Let us see how 
far this gets us. 
For example, suppose we are given $\varphi$ by a matrix 
\begin{align*}
    \Phi & = 
    \begin{bmatrix}
        1 & 1 & 0 & 3\\
        1 & 0 & 2 & 5\\
        1 & 1 & 0 & 3
    \end{bmatrix}
\end{align*}
So what is in $\ker\varphi$?  The set 
offers no answer.

% Thus any properties of $\ker\varphi$ will need to be 
% developed from within that system. There are two complications. 
% First most persons, and a positive proportion of mathematicians,
% cannot state the rules for sets.  Second, the full 
% spectrum of set properties is well-beyond what can be computed 
% or proved consistent.  In fact, even the fundamental axioms 
% are not a finite list but must instead involve axioms schema
% and ramification.  There is nothing wrong with using these 
% properties, unless you don't know what any of that means.
% But we are not motivated to explain laws and logic of all 
% sets, we simply want to understand and use kernels.  So 
% what follows are alternative definitions that can each 
% be described in a handful of precise rules and are amenable 
% to computing.

\subsection{Kernels in Computer Algebra Systems}
We can begin by inputting a linear mapping (as a matrix) into a computer algebra
system.
\begin{notebookin}
Phi = [ 1 1 1; 1 0 1; 0 2 0; 3 5 3 ]
print Phi
\end{notebookin}
\begin{notebookout}[\thenotebookcounter]
[ 1 1 0 3 ]
[ 1 0 2 5 ]
[ 1 1 0 3 ]
\end{notebookout}
Research shows humans make 3-6 errors per hour, no matter 
what the task is.  Why waste any of 
them on miscalculating?  Let us ask a computer for the kernel.
\begin{notebookin}
K = Kernel(Phi)
print K
\end{notebookin}
\begin{notebookout}[\thenotebookcounter]
[ -2 -5 ]
[  2  2 ]
[  1  0 ]
[  0  1 ]
\end{notebookout}
This may be a surprise.  This is a matrix, not a 
set.  Why?

\begin{prob}
    Find access to a computer algebra system 
    and compute kernels of random $1000\times 1200$, 
    $1000 \times 1000$ and $1200\time 1000$ matrices.
\end{prob}

\subsection{Kernels in Diagram language}
To decode the difference between the set-wise concept of a kernel 
and what a program provides, we start with an alternative definition of kernels 
using diagrams and focussed on what it means to put data into the kernel
and to get data out of the kernel.

We start with a linear map, and so that gives us our first diagram.
It is introduced with $\forall$ because it applies to \emph{all} linear maps.
We should also indicate somewhere that the context is modules, which 
we do once at the start by writing ${_{\Omega} \mathsf{Mod}}$.  Often 
we authors skip that step letting context be know implicitly.  In any case
the figure we start with is below.
\begin{center}
    \begin{tikzpicture}
        %% For All
        \node[outer sep=5pt] (form) at (0,0) 
        {\begin{tikzcd}[background color=black!15,column sep=tiny]
            W & \phantom{J} & V \arrow[ll,"\varphi"{above}]
        \end{tikzcd}};
    
        \node (all) at (form.north west) {$\forall$};
        \draw[thick] (all.south) -- (form.south west);

        \node (text) at (form.north) {${_{\Omega}\mathsf{Mod}}$};
    \end{tikzpicture}       
\end{center}

Kernels exist for every linear map and will appear as a new structure denoted 
$\ker\varphi$.  This on its own would have no relation to $\varphi$ in the diagram, so we draw out that relationship by adding new arrows, new 
linear functions that is.  One arrow is 
just $0$, and the second $\iota$ is injective, displayed with a hook.  
\begin{center}
    \begin{tikzpicture}
    %% There exists
    \node[outer sep=5pt, fill=black!15] (form) at (4,0) 
    {\begin{tikzcd}[
        background color=black!15,
        column sep=tiny
        ]
        W & & V\arrow[ll,"\varphi"{above}]\\
            & \ker\varphi\arrow[lu,"0"{below}]\arrow[ur,hook,"\iota"{below}]
    \end{tikzcd}};

    \node (exists) at (form.north west) {$\exists$};
    \draw[thick] (exists.south) -- (form.south west);
\end{tikzpicture}  
\end{center}

The $0$ arrow should be explained a bit, it just means $0(x)=x$.  
This is an uneventful step but its role will unfold as necessary because it will
offer the constraint to the equations that follow.  Without it there would be no
equations and thus no solutions.  Note, some authors prefer to take two steps
for zero functions pausing to pass through the zero module $\{0\}$.  Somewhat
confusingly that space is also written as $0$. Get used to it but prepared to
explain your own missuses of $0$ should anyone ask you. So instead of writing
$0:A\to B$ some authors will write a sequence of arrows $A\to 0\to B$.

The real star is the arrow denoted by $\iota$ which feeds into the arrow
$\varphi$.  Lining up the arrows indicates we can compose these two functions
$\varphi\circ \iota$.  This should equal the other arrow reaching the same
point, namely $0$.  So $\varphi\circ \iota=0$, or rather $\varphi(\iota(k))=0$
for $k\in \ker\varphi$.  We say the diagram \emph{commutes} and we indicate this
by shading the background.

The $\iota$ in this diagram is a new linear map, and so it could be 
given by a matrix.  In the example given earlier, the matrix 
$\Phi$ produced a kernel function $\iota$ whose matrix was the matrix
that our computer produced.  So through our new lens of kernels the 
computer output is correct, we should get a matrix not a set.

Unfortunately we cannot stop here even though we have the promised kernel. This
is because many spaces $K$ and functions $\kappa:K\to V$ could play the role of
$\ker\varphi$ as shown in the above diagram without actually being the kernel we
have in mind.  For example, $K=\{0\}$ certainly would do the same but for the matrix
$\Phi$ given earlier we expect a different answer.  So this cannot be a complete
understanding of kernels.  We need $\ker\varphi$ to be ``as big as possible''.
Even saying that we find a puzzle because what would it mean for a function to
be ``as big as possible''.  To resolve this let us add to our diagram any other
data and functions that can match what we already know about kernels.
\begin{center}
    \begin{tikzpicture}
    %% For All constraints
    \node[outer sep=5pt, fill=black!15] (constraint) at (8,0) 
    {\begin{tikzcd}[background color=black!15,column sep=tiny]
        W & & V\arrow[ll,"\varphi"{above}]\\
         & \ker\varphi\arrow[lu,"0"{below}]\arrow[ur,hook,"\iota"{below}]\\
         & K \arrow[uul,bend left,"0"{below}]\arrow[uur, hook, bend right, "\kappa"{below}]
    \end{tikzcd}};

    \node (all2) at (constraint.north west) {$\forall$};
    \draw[thick] (all2.south) -- (constraint.south west);
        
    \end{tikzpicture}
\end{center}
The quantifier $\forall$ here now ranges over $\kappa:K\to V$ so we are 
setting ourselves up to compare how our chosen $\ker\varphi$ compares to 
any other possible solution.

The conclusion you might have guessed is that our solution should be 
at least as capable as any other, and to diagram that we simply need that 
every alternative solution can be mapped into our own $\ker \varphi$.  So there
exists a unique new arrow $K\to \ker\varphi$ transforming any competitor data 
into data of our own type, as shown below.
\begin{center}
\begin{tikzpicture}
    %% exist unique
    \node[outer sep=5pt, fill=black!15] (universal) at (12,0) 
    {\begin{tikzcd}[background color=black!15,column sep=tiny]
        W & & V\arrow[ll,"\varphi"{above}]\\
         & \ker\varphi\arrow[lu,"0"{below}]\arrow[ur,hook,"\iota"{below}]\\
         & K \arrow[uul,bend left,"0"{below}]\arrow[uur, hook, bend right, "\kappa"{below}]\arrow[u]
    \end{tikzcd}};

    \node (unique) at (universal.north west) {$\exists !$};
    \draw[thick] (unique.south) -- (universal.south west);
    
\end{tikzpicture}    
\end{center}

Like frames in a graphic novel, these four diagrams should be read 
as a timeline, see Figure~\ref{fig:kernel-diag}.  
\begin{figure}[!htbp]
\begin{center}
    \includegraphics[width=\textwidth]{kernel-graphic.pdf}
\end{center}
\caption{The diagram description of kernels.}
\label{fig:kernel-diag}
\end{figure}

This pattern of 
``$\forall \exists\forall\exists!$'' will be repeated many times in similar 
constructions.  In fact the longer we work with logical puzzles we will 
find the steady use of the pattern 
\begin{align*}
    \Pi_n & \equiv \overbrace{\forall \exists\cdots \forall \exists}^n
    & 
    \Sigma_n & \equiv \overbrace{\exists\forall\cdots \exists\forall}^n.
\end{align*}
These organizations of logical sentences was introduced by Kleene and 
Mostowski and is known today as the \emph{Arithmetical Hierarchy}.
Programmers who ask how hard it is to prove statements in that Hierarchy 
will find they have invented the \emph{Polynomial-time Hierarchy}.
So it is worth getting comfortable with the meaning, but be warned 
that answering questions in these hierarchies could earn you a million Euro 
prize and your name in the newspaper.  One of the leading questions 
is if this tower may one day collapse, meaning that you only need to go 
to some fixed value of $n$ before you know everything.

\begin{prob} 
    Prove that any two kernels are isomorphic.
\end{prob}




\subsection{Kernels in Typed language}

The diagram language clarifies how kernels can be thought of as functions and 
functions that have a maximal quality.  Yet, programs do not think in pictures, 
we do.  So we need to translate the same ideas to a syntax we can turn into a 
program.  Here is how.

First the question depends on $\Omega$-modules $V$ and $W$ and a linear map $\varphi:V\to W$ 
known first from context, denoted $\mathsf{ctx}$ or $\Gamma$.  Using the notation 
$P\vdash Q$ to say ``$P$ leads to $Q$'', also denoted $\frac{P}{Q}$, then we can 
state this as forming the kernel under a list of assumed knowledge.
\begin{gather}
    \tag{$\form{\ker}$}
    \frac{
        \mathsf{ctx} \vdash V,W:{_{\Omega} \mathsf{Mod}}\qquad
        \varphi:\mathsf{Lin}_{\Omega}(V,W)
    }{
        \mathsf{ctx}\vdash \ker\varphi:\mathsf{Type}
    }
\end{gather}
The label $\form{\ker}$ stands for \emph{formation}.  Programs write this in
many different ways usually by introducing some keywords like ``import''
and ``use X from Y'.  To introduce a new type of data a keyword such as ``class''
or ``type'' is used.  For example, the following pseudo-code reflects the 
content of $(\form{\ker})$ but in a dialect similar to several modern 
procedural programming 
languages such as C++ and Java.
\begin{lstlisting}[language=Sava]
using V,W:Mod[Omega], Phi:Lin[V,W] from ctx
class Ker[Phi] {...}
\end{lstlisting}
For those using functional programming languages like OCaml or Haskell the following syntax offers 
as similar translation.
\begin{lstlisting}[language=Hidris]
import V,W:Mod Omega, Phi:Lin V W from ctx
type Ker Phi
\end{lstlisting}



\begin{lstfloat}[!htbp]
\begin{lstlisting}[language=Sava]
// Procedural style code
class Ker[Phi](k:K,kappa:K->V) where (Phi(kappa(k)) == 0)
// usage 
Phi = ...; k = ...; kappa = ...;
x = new Ker[Phi](k,kappa)
\end{lstlisting}
\begin{lstlisting}[language=Hidris]
--- Functional style code
type Ker Phi
null: (k:K)-> (kappa:K->V)-> (Phi kappa k == 0)-> Ker Phi
--- usage 
Phi= ...; k= ...; kappa= ...;
x= null Phi k kappa --- system checks Phi kappa k == 0
\end{lstlisting}    
\caption{An introduction of data to a kernel.}
\label{lst:kernel-intro}
\end{lstfloat}
    
Next the diagram above captured the high-level movement of that data without 
ever considering the actual data.  The programs will certainly need these 
data.  The premise from the diagram is that any data $k:K$ which is found to 
have $\kappa(k):V$ where $\varphi(\kappa(j))=0$ (see the diagrams above) must 
produce data in $\ker\varphi$.
Any such data $k:K$ is meant to produce data in the kernel, because the 
kernel is the largest such structure.  So we include such a rule.
\begin{gather}
    \tag{$\intro{\ker}$}
    \frac{
        k:K\qquad \kappa:K\to V\qquad pf:\varphi(\kappa(k))=_W 0
    }{
        \mathsf{null}(\kappa(k)):\ker\varphi
    }
\end{gather}
The $\intro{\ker}$ here is for \emph{introduction} because data is 
being introduced of the desired type.  
Most readers will not be prepared for the meaning of symbols like:
\[
    pf:\varphi(\kappa(k))=_W 0
\] 
Programmers however are uniquely well-positioned to guess the meaning. We want
some data $pf$ that has the type $\varphi(\kappa(k))=0$ in $W$. Said another
way, we need someone to provide a proof of that equality. In programs this can
be done by several tricks most common are what are known as \emph{guards} or
\emph{rails}.  These are a type of documentation added to a program to let the
programming language enforce that data is used in restricted ways.  In this
case, no one can introduce a term in the kernel without proof. In code this can
be captured in a number of ways, Listing~\ref{lst:kernel-intro} is one option.

% The introduction of data of some kind is known 
% to programmers as a \emph{constructor} and many languages make special 
% rules to specify constructors.  For uniformity and simplicity we here 
% use `def' to introduce all service functions to a data type and name 
% the function to match the notation used in the mathematical formalism.
% But any real program will adapt the vocabulary and style to fit with 
% conventions.


Now it is time to use data in the kernel.  It is clear how this should 
proceed, anything in the kernel can be mapped to $0$ in $W$ or to a 
value in $V$ which will map to $0$ under $\varphi$.  The rules are therefore 
as follows.    
\begin{gather}
    \tag{$\elim{\ker}$}
    \frac{
        x:\ker\phi
    }{
        0:W
    }\qquad
    \frac{
        x:\ker\phi
    }{
        \iota(x):V
    }
\end{gather}
The name $\elim{\ker}$ stands for \emph{elimination} as we are eliminating 
the kernel type to get to new types.
In code this might be done as shown in the code fragment Listing~\ref{lst:kernel-elim}.
\begin{lstfloat}[!htbp]
\begin{lstlisting}[language=Sava]
// Procedural style code
class Ker[Phi](k:K,kappa:K->V) where (Phi(kappa(k))== 0){
    def iota:V = ...
    def zero:W = ...
}
// usage 
x = new Ker[Phi](k,kappa)
v = x.iota
\end{lstlisting}
\begin{lstlisting}[language=Hidris]
--- Functional style code
iota: Ker Phi -> V
...
zero: Ker Phi -> W
...
--- usage 
x= null Phi k kappa --- system checks Phi kappa k == 0
v = iota x
\end{lstlisting}    
\caption{Using of data of kernel type.}
\label{lst:kernel-elim}
\end{lstfloat}

Finally we need to do some computing somewhere and we learn 
what to compute by inspecting the condition of ``commutative diagrams''.
\begin{gather}
    \tag{$\comp{\ker}$}
    \frac{
        k:K\qquad \kappa:K\to V\qquad pf:\varphi(\kappa(j))=_W 0
    }{
        \iota(x) \defeq \kappa(k)
    }
\end{gather}
All together this comes together in software in many different ways 
each designed around different techniques to improve how we read and 
execute code.  Listing~\ref{lst:kernel-comp} provides some of the options.
\begin{lstfloat}[!htbp]
\begin{lstlisting}[language=Sava]
// Procedural style code
class Ker[Phi](k:K,kappa:K->V) where (Phi(kappa(k))== 0){
    def iota:V = kappa(k)
    def zero:W = 0
}
// usage 
x = new Ker[Phi](k,kappa)
v = x.iota
\end{lstlisting}
\begin{lstlisting}[language=Hidris]
--- Functional style code
iota: Ker Phi -> V
iota x = kappa k where x = null k kappa
zero: Ker Phi -> W
zero x = 0
--- usage 
x= null Phi k kappa --- system checks Phi kappa k == 0
v = iota x
\end{lstlisting}    
\caption{Complete data type for kernels.}
\label{lst:kernel-comp}
\end{lstfloat}
% \begin{lstfloat}
% \begin{lstlisting}[language=Sava]
% using V,W:Mod[Omega], Phi:Lin[V,W] from ctx
% class Ker[Phi] {
%     private v:V

%     def null(j:J, kappa:J->V, 
%         require Phi(kappa(j)) == 0) {
%         v = kappa(j)    
%     }
%     def iota:V = v
%     def zero:W = 0
% }
% \end{lstlisting}
% \caption{A complete data type for kernels}    
% \label{lst:kernel-comp}
% \end{lstfloat}



\end{document}