
\documentclass[12pt,twoside,dvipsnames,letterpaper]{memoir}
\input{tex-preambles/Content.tex}
\input{tex-preambles/tikz-sets.tex}
\input{tex-preambles/math-preambles.tex}
\input{tex-preambles/listings-preamble.tex}

\usepackage{CJKutf8}
\author{James B. Wilson}
\date{\today}
\begin{document}

\chapter{The Four Spaces}


\begin{table}[!htbp]
    \centering
    \includegraphics{4-space-table.pdf}
    \caption{The 4 spaces of a linear map.}
    \label{tab:4-spaces}
\end{table}
\begin{CJK*}{UTF8}{gbsn}

\subsection{ 方程 ~Fang cheng}
\end{CJK*}
Algorithms appearing in China around 2000 years
ago depict the use of linear algebra on matrices 
to solve problems.  The Chinese called these matrices 
\emph{F\u{a}ng} \emph{ch\'eng}.
A thousand years later in Iraq and Iran where inventing algebra
and using it to solve systems of linear equations.
In the late 1600's Leibniz recorded matrix ideas for European 
scholars and may have even been aware of the Chinese Fang cheng
before doing so.  
A number of textbooks today call this process
``Gaussian Elimination'' after the 19th century math polymath 
Carl Friedrich Gauss who systemized the mechanics.  
Now that the Chinese origins are better known the best 
practice in history would promote this attribution instead.


\begin{definition}
    For a natural number $d$, $[d]=\{1,\ldots,m\}$ with $[0]$
    the empty set.
    \begin{itemize}
        \item
    A \emph{coordinate vector} is function $v:[d]\to \Delta$
    also denoted $v:\Delta^d$.  We access its $i$-th value by 
    writing $v_i$.

    \item 
    An \emph{$d_1\times d_2$-matrix} $\Phi$ over $\Delta$
    is a function $\Phi:[d_1]\times [d_2]\to \Delta$.
    These are also denoted $\Phi:\Delta^{d_1\times d_2}$.
    Evaluating that function as $(i,j)$ is denoted 
    $\Phi_{ij}$.  The coordinates $i$ are called \emph{rows}
    and the $j$'s are called \emph{columns}.

    \item A \emph{coordinate tensor} (also called a \emph{multiway array})
     is function 
    $t:[d_1]\times \cdots\times [d_{\ell}]\to \Delta$.
    The $i:[\ell]$ are called \emph{axes}.
    \end{itemize}
\end{definition}

Of the many important classes of of matrices.
\begin{align}
    \tag{$e_i$}
    e_i&:\Delta^n  & (e_i)_j  & = \left\{\begin{array}{cc}
        1 & i=j\\ 0 & \text{else}
    \end{array}\right.\\
    \tag{$E_{ij}$}
    E_{ij}& :\Delta^{m\times n}  
    &
    (E_{ij})_{kl} &= \left\{\begin{array}{cc}
        1 & i=k, j=l\\
        0 & \text{else}
    \end{array}\right.
\end{align}
These are sometimes called \emph{matrix units}.
For example for $m=3,n=4$ we have
\[
    E_{23}=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}
    \qquad
    E_{22}=\begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}
    \qquad 
    E_{32}
    =\begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}
\]
\begin{proposition}
    Every $(m\times n)$-matrix $\Phi$ is the sum of scaled matrix units,
    \[
        \Phi = \sum_{i}\sum_j \Phi_{ij}E_{ij}.
    \]
\end{proposition}

For $m=n$ we have the following essential matrices.
\begin{gather}
    \tag{$I_n$}
    (I_n)_{ij} = \left\{\begin{array}{cc}
        1 & i=j\\
        0 & \text{else}
    \end{array}\right.\\
    \tag{Transvection}
    I_n+\alpha E_{ij}  \quad i\neq j\\
    \tag{Diagonal}
    Diag(a_1,\ldots,a_n)_{ij} = \left\{\begin{array}{cc}
        a_i & i=j\\
        0 & \text{else}
    \end{array}\right.\\
    \tag{Permutation}
    \Sigma_{ij}\neq 0  \Rightarrow (\Sigma_{ij}=1) \& (k\neq i\Rightarrow \Sigma_{kj}=0)
    \& (j\neq k\Rightarrow \Sigma_{ik}=0).
\end{gather}
These last three families are known as \emph{elementary matrices}.
Each is invertible. 

\section{Clearing a column}


Consider what we might do to clear a column in the following matrix.
\begin{align*}
    \begin{bmatrix}
        4 & 12 \\
        6 & 30 \\
        27 & 24
     \end{bmatrix}
\end{align*}
Ready...set...WAIT!  Lets examine what rules we are prepared to except.

The first moves in linear algebra are to organize the data, swapping 
what can be swapped.  Think of a larger chalkboard with the numbers in 
place.  Would you really want the chore of copying the data over just 
to move it?  Computers likewise rarely move the data either.  Instead 
they just compose the function $M:[d_1]\times [d_2]\to \Delta$ 
with permutations of $[d_1]$, respectively $[d_2]$ and leave the data 
exactly as it was but allow the user of the matrix to interact with the 
data as if it was moved.  Said another way, lets just leave permutations 
off the menu as they are mostly cosmetic.

The next move in linear algebra is to turn a nonzero into a $1$ so 
we can use it as a pivot.  Indeed in Linear Algebra we saw by 
Schur's lemma that the data can be written with coefficients in 
a division ring.  So nonzero values have inverses.  But there 
are two reasons to question this move.  When we rescale we can 
build really small or really large numbers and this causes rounding 
problems.  Addition is far more stable.  Secondly, there are numerous 
applications where numbers are restricted and division is 
not permitted.  So let us set aside rescaling until we need it.

Subtract $2\times$ row 1 to from row 2 and once from row 3.
\begin{align*}
    \begin{bmatrix}
        1 & 0 & 0 \\
        -1 & 1 & 0 \\
        -6 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
        4 & 12\\
        6 & 30 \\
        27 & 24
     \end{bmatrix}
     & = 
     \begin{bmatrix}
        4 & 12 \\
        2 & 18 \\
        3 & -48 
     \end{bmatrix}
\end{align*}
Subtract row 2 from row 3.
\begin{align*}
     \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & -1 & 1
    \end{bmatrix}
     \begin{bmatrix}
        4 & 12 \\
        2 & 18 \\
        3 & -48 
     \end{bmatrix}
     & = 
     \begin{bmatrix}
        4 & 12 \\
        2 & 18 \\
        1 & -64 
     \end{bmatrix}
\end{align*}
Use row 3 to subtract from rows 2 and 4.
\begin{align*}
     \begin{bmatrix}
        1 & 0 & -4 \\
        0 & 1 & -2 \\
        0 & 0 & 1
    \end{bmatrix}
     \begin{bmatrix}
        4 & 12 \\
        2 & 18 \\
        1 & -64 
     \end{bmatrix}
     & = 
     \begin{bmatrix}
        0 & 268\\
        0 & 146\\
        1 & -64 
     \end{bmatrix}
\end{align*}
It took more work but we achieved the goal of clearing 
the first column with no division and no permutations.

Now lets try to clear one more column using only transvections. 
We suppress these matrices.
\begin{align*}
    \begin{bmatrix}
        4\\ 6\\ 12
    \end{bmatrix}
    \sim 
    \begin{bmatrix}
        4\\ 2\\ 0
    \end{bmatrix}
    \sim
    \begin{bmatrix}
        0\\ 2\\ 0
    \end{bmatrix}
\end{align*}
Here we have to stop because we have refused to divide.
This could of course just indicated that we tried the 
wrong strategy.  Or it could be a symptom of an obstacle.
Which is it?  Well notice the following.
\begin{align*}
    \begin{bmatrix}
        4\\ 6\\ 12
    \end{bmatrix}
    =
    2\begin{bmatrix}
        2\\ 3\\ 6
    \end{bmatrix}
\end{align*}
So whatever our transformations $Tv=T(2u)=2(Tu)$.
So the result will 
be a multiple of 2.  So unless it is all $0$ we are at our 
best to get to $2e_i$.  This is a general principal.

\begin{proposition}
    If $v=\lambda u$ and $T$ is a product of transvections
    then $Tv$ is a multiple of $\lambda$.
\end{proposition}

With this limitation in mind we can consider writing 
all the values in the vector with no common divisor
(meaning if $\lambda \mid v_i$ for each $i$ then 
$\lambda$ is invertible).
This is not possible in all number systems but that 
is for another course entirely.
Let us make one further assumption that there is an 
algorithm to compute greatest common divisors (the 
Euclidean algorithm will do).  This also allows us 
to find a vector $u$ where $u\cdot v=1$.  

For example, $v=(5,17)$ then we may use $u=(5,-2)$ 
so that $7\cdot 5+-2\cdot 17=1$.
Using this we have a means to clear.
\begin{align*}
    \begin{bmatrix}
        1 & 0 \\
        -2 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        1 & -2 \\
        0 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        1 & 0 \\
        -3 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        5 \\
        17 \\
    \end{bmatrix}
     = 
    \begin{bmatrix}
        1 & 0 \\
        -2 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        1 & -2 \\
        0 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        5 \\ 2
    \end{bmatrix}
     =
    \begin{bmatrix}
        1 & 0 \\
        -2 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        1 \\ 2
    \end{bmatrix}
    = \begin{bmatrix}
        1 \\ 0 
    \end{bmatrix}
\end{align*}
Now let us multiply these matrices together.
\begin{align*}
    \begin{bmatrix}
        1 & 0 \\
        -2 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        1 & -2 \\
        0 & 1
    \end{bmatrix}    
    \begin{bmatrix}
        1 & 0 \\
        -3 & 1
    \end{bmatrix}    
    = 
    \begin{bmatrix}
        1 & -2\\
        -2 & 5
    \end{bmatrix}
    \begin{bmatrix}
        1 & 0 \\
        -3 & 1
    \end{bmatrix} 
    =
    \begin{bmatrix}
        7 & -2\\
        -17 & 5   
    \end{bmatrix}.
\end{align*}
Now we see the anatomy of this solution.  Because $u\cdot v=1$
we put $u$ in the first row.  Then we simply write down a basis 
of vector perpendicular to $v$ for the remaining rows.

\begin{definition}
    A matrix is \emph{unimodular} if it is a product 
    of transvections.  
\end{definition}

\begin{proposition}
    If we have a Euclidean algorithm then 
    for every nonzero $v:\Delta^n$ there is a 
    unimodular matrix $T$, a scalar $\lambda$,
    and $i:[n]$ such that $Tv=\lambda e_i$.
\end{proposition}

\subsection{Tableaux and Normal Forms}
We all know the Reduced Row Echelon Form (RREF).  
This is merely a recursive use of clearing columns as above.
But it would help not to go in circles and have a target.


\begin{definition}
    A matrix $\Phi$ is in \emph{Reduce Row Echelon Form (RREF)}
    if there is a permutation matrix $\Sigma$ of the columns such 
    that 
    \begin{align*}
        \Phi\Sigma & = \begin{bmatrix}
            I_r & M \\
            0 & 0 
        \end{bmatrix}.
    \end{align*}
    The \emph{Hermite Normal Form} is a permutation matrix 
    $\Sigma$ and numbers $a_1|\cdots |a_r$ such that 
    \begin{align*}
        \Phi\Sigma & = \begin{bmatrix}
            D & M \\
            0 & 0 
        \end{bmatrix}
        & 
        D & = \mathsf{Diag}(a_1,\ldots,a_r).
    \end{align*}
\end{definition}


So how can we make this outcome?  
Augment the matrix $\Phi$ by $I_m$ to create a tableaux
$T=[I_m | \Phi]$.  
The first $m$ columns are called \emph{artificial}.
Now we scan the $\Phi$ for 
a column $j$ which we can clear.  So if it is $0$ we skip it, 
and if not we scan for the column whose greatest common 
divisor divides the greatest common divisor of the entire matrix.
Then repeat on the remaining basic columns.

Let us look at this in the case of a division ring
we clear the column with pivot in row 1.
\begin{align*}
    \begin{array}{|ccc|ccc|c|ccc|}
        \hline
        & & &  && & j & & & \\
        \hline 
        1 & & & & & & v_1 &  & & \\
        & \ddots & & &  *  & & \vdots & & * & \\
        & & 1 & & & &v_m & & & \\
        \hline
    \end{array}
\end{align*}
becomes
\begin{align*}
    \begin{array}{|ccc|ccc|c|ccc|}
        \hline
        & & &  && & j & & & \\
        \hline 
        * & & & & & & 1 &  & & \\
        * & \ddots & & &  *  & & 0 & & * & \\
        \vdots & & 1 & & & & \vdots & & & \\
        \hline
    \end{array}
\end{align*}
Because we had a pivot in row 1 the clear occurred only from row 1
and so only the first column was affected.  It is said that the 
first column is \emph{leaving} and the $j$-th column is \emph{entering}.
We proceed with every artificial column.  The columns that enter 
become the identity of the RREF.

\begin{proposition}
    Every matrix over a division ring has an RREF.
    In the general ring case with Euclidean algorithm 
    there is a Hermite Normal Form.
\end{proposition}



\section{Kernels}

\subsection{Kernels in Set language}
Given $\Omega$-modules $V$ and $W$ and a linear map $\varphi:V\to W$, 
\[
    \ker \varphi=\{v\in V\mid \varphi(v)=0\}.
\]
This definition is concise because it's meaning
is outsourced to the concept of a set. Let us see how 
far this gets us. 
For example, suppose we are given $\varphi$ by a matrix 
\begin{align*}
    \Phi & = 
    \begin{bmatrix}
        1 & 1 & 0 & 3\\
        1 & 0 & 2 & 5\\
        1 & 1 & 0 & 3
    \end{bmatrix}
\end{align*}
So what is in $\ker\varphi$?  The set 
offers no answer.

% Thus any properties of $\ker\varphi$ will need to be 
% developed from within that system. There are two complications. 
% First most persons, and a positive proportion of mathematicians,
% cannot state the rules for sets.  Second, the full 
% spectrum of set properties is well-beyond what can be computed 
% or proved consistent.  In fact, even the fundamental axioms 
% are not a finite list but must instead involve axioms schema
% and ramification.  There is nothing wrong with using these 
% properties, unless you don't know what any of that means.
% But we are not motivated to explain laws and logic of all 
% sets, we simply want to understand and use kernels.  So 
% what follows are alternative definitions that can each 
% be described in a handful of precise rules and are amenable 
% to computing.

\subsection{Kernels in Computer Algebra Systems}
We can begin by inputting a linear mapping (as a matrix) into a computer algebra
system.
\begin{notebookin}
Phi = [ 1 1 1; 1 0 1; 0 2 0; 3 5 3 ]
print Phi
\end{notebookin}
\begin{notebookout}[\thenotebookcounter]
[ 1 1 0 3 ]
[ 1 0 2 5 ]
[ 1 1 0 3 ]
\end{notebookout}
Research shows humans make 3-6 errors per hour, no matter 
what the task is.  Why waste any of 
them on miscalculating?  Let us ask a computer for the kernel.
\begin{notebookin}
K = Kernel(Phi)
print K
\end{notebookin}
\begin{notebookout}[\thenotebookcounter]
[ -2 -5 ]
[  2  2 ]
[  1  0 ]
[  0  1 ]
\end{notebookout}
This may be a surprise.  This is a matrix, not a 
set.  Why?


\subsection{Kernels in Diagram language}
To decode the difference between the set-wise concept of a kernel 
and what a program provides, we start with an alternative definition of kernels 
using diagrams and focussed on what it means to put data into the kernel
and to get data out of the kernel.

We start with a linear map, and so that gives us our first diagram.
It is introduced with $\forall$ because it applies to \emph{all} linear maps.
We should also indicate somewhere that the context is modules, which 
we do once at the start by writing ${_{\Omega} \mathsf{Mod}}$.  Often 
we authors skip that step letting context be know implicitly.  In any case
the figure we start with is below.
\begin{center}
    \begin{tikzpicture}
        %% For All
        \node[outer sep=5pt] (form) at (0,0) 
        {\begin{tikzcd}[background color=black!15,column sep=tiny]
            W & \phantom{J} & V \arrow[ll,"\varphi"{above}]
        \end{tikzcd}};
    
        \node (all) at (form.north west) {$\forall$};
        \draw[thick] (all.south) -- (form.south west);

        \node (text) at (form.north) {${_{\Omega}\mathsf{Mod}}$};
    \end{tikzpicture}       
\end{center}

Kernels exist for every linear map and will appear as a new structure denoted 
$\ker\varphi$.  This on its own would have no relation to $\varphi$ in the diagram, so we draw out that relationship by adding new arrows, new 
linear functions that is.  One arrow is 
just $0$, and the second $\iota$ is injective, displayed with a hook.  
\begin{center}
    \begin{tikzpicture}
    %% There exists
    \node[outer sep=5pt, fill=black!15] (form) at (4,0) 
    {\begin{tikzcd}[
        background color=black!15,
        column sep=tiny
        ]
        W & & V\arrow[ll,"\varphi"{above}]\\
            & \ker\varphi\arrow[lu,"0"{below}]\arrow[ur,hook,"\iota"{below}]
    \end{tikzcd}};

    \node (exists) at (form.north west) {$\exists$};
    \draw[thick] (exists.south) -- (form.south west);
\end{tikzpicture}  
\end{center}

The $0$ arrow should be explained a bit, it just means $0(x)=x$.  
This is an uneventful step but its role will unfold as necessary because it will
offer the constraint to the equations that follow.  Without it there would be no
equations and thus no solutions.  Note, some authors prefer to take two steps
for zero functions pausing to pass through the zero module $\{0\}$.  Somewhat
confusingly that space is also written as $0$. Get used to it but prepared to
explain your own missuses of $0$ should anyone ask you. So instead of writing
$0:A\to B$ some authors will write a sequence of arrows $A\to 0\to B$.

The real star is the arrow denoted by $\iota$ which feeds into the arrow
$\varphi$.  Lining up the arrows indicates we can compose these two functions
$\varphi\circ \iota$.  This should equal the other arrow reaching the same
point, namely $0$.  So $\varphi\circ \iota=0$, or rather $\varphi(\iota(k))=0$
for $k\in \ker\varphi$.  We say the diagram \emph{commutes} and we indicate this
by shading the background.

The $\iota$ in this diagram is a new linear map, and so it could be 
given by a matrix.  In the example given earlier, the matrix 
$\Phi$ produced a kernel function $\iota$ whose matrix was the matrix
that our computer produced.  So through our new lens of kernels the 
computer output is correct, we should get a matrix not a set.

Unfortunately we cannot stop here even though we have the promised kernel. This
is because many spaces $K$ and functions $\kappa:K\to V$ could play the role of
$\ker\varphi$ as shown in the above diagram without actually being the kernel we
have in mind.  For example, $K=\{0\}$ certainly would do the same but for the matrix
$\Phi$ given earlier we expect a different answer.  So this cannot be a complete
understanding of kernels.  We need $\ker\varphi$ to be ``as big as possible''.
Even saying that we find a puzzle because what would it mean for a function to
be ``as big as possible''.  To resolve this let us add to our diagram any other
data and functions that can match what we already know about kernels.
\begin{center}
    \begin{tikzpicture}
    %% For All constraints
    \node[outer sep=5pt, fill=black!15] (constraint) at (8,0) 
    {\begin{tikzcd}[background color=black!15,column sep=tiny]
        W & & V\arrow[ll,"\varphi"{above}]\\
         & \ker\varphi\arrow[lu,"0"{below}]\arrow[ur,hook,"\iota"{below}]\\
         & K \arrow[uul,bend left,"0"{below}]\arrow[uur, hook, bend right, "\kappa"{below}]
    \end{tikzcd}};

    \node (all2) at (constraint.north west) {$\forall$};
    \draw[thick] (all2.south) -- (constraint.south west);
        
    \end{tikzpicture}
\end{center}
The quantifier $\forall$ here now ranges over $\kappa:K\to V$ so we are 
setting ourselves up to compare how our chosen $\ker\varphi$ compares to 
any other possible solution.

The conclusion you might have guessed is that our solution should be 
at least as capable as any other, and to diagram that we simply need that 
every alternative solution can be mapped into our own $\ker \varphi$.  So there
exists a unique new arrow $K\to \ker\varphi$ transforming any competitor data 
into data of our own type, as shown below.
\begin{center}
\begin{tikzpicture}
    %% exist unique
    \node[outer sep=5pt, fill=black!15] (universal) at (12,0) 
    {\begin{tikzcd}[background color=black!15,column sep=tiny]
        W & & V\arrow[ll,"\varphi"{above}]\\
         & \ker\varphi\arrow[lu,"0"{below}]\arrow[ur,hook,"\iota"{below}]\\
         & K \arrow[uul,bend left,"0"{below}]\arrow[uur, hook, bend right, "\kappa"{below}]\arrow[u]
    \end{tikzcd}};

    \node (unique) at (universal.north west) {$\exists !$};
    \draw[thick] (unique.south) -- (universal.south west);
    
\end{tikzpicture}    
\end{center}

Like frames in a graphic novel, these four diagrams should be read 
as a timeline, see Figure~\ref{fig:kernel-diag}.  
\begin{figure}[!htbp]
\begin{center}
    \includegraphics[width=\textwidth]{kernel-graphic.pdf}
\end{center}
\caption{The diagram description of kernels.}
\label{fig:kernel-diag}
\end{figure}

This pattern of 
``$\forall \exists\forall\exists!$'' will be repeated many times in similar 
constructions.  In fact the longer we work with logical puzzles we will 
find the steady use of the pattern 
\begin{align*}
    \Pi_n & \equiv \overbrace{\forall \exists\cdots \forall \exists}^n
    & 
    \Sigma_n & \equiv \overbrace{\exists\forall\cdots \exists\forall}^n.
\end{align*}
These organizations of logical sentences was introduced by Kleene and 
Mostowski and is known today as the \emph{Arithmetical Hierarchy}.
Programmers who ask how hard it is to prove statements in that Hierarchy 
will find they have invented the \emph{Polynomial-time Hierarchy}.
So it is worth getting comfortable with the meaning, but be warned 
that answering questions in these hierarchies could earn you a million Euro 
prize and your name in the newspaper.  One of the leading questions 
is if this tower may one day collapse, meaning that you only need to go 
to some fixed value of $n$ before you know everything.


\subsection{Computing the kernel}
The description above achieved the idea of kernel without clearifying 
how to get it.  We can return to our RREF to get that answer.
Let us consider a matrix in which a subset of the columns 
are an identity matrix.  That is, up to possibly permuting 
the columns the matrix has the form
\begin{align*}
    \begin{bmatrix}
        I_r & M \\ 
        0 & 0 
    \end{bmatrix}\in \Delta^{m\times n}
\end{align*}
Then an answer would be written down with formula 
requiring no computation:
\begin{align*}
    \begin{bmatrix}
        I_r & M \\ 
        0 & 0 
    \end{bmatrix}
    \begin{bmatrix}
        -M  \\ 
        I_{n-r} 
    \end{bmatrix}
    & = 
    \begin{bmatrix}
        0\\
        0
    \end{bmatrix}
\end{align*}


\begin{proposition}
    For every $(m\times n)$-matrix $\Phi$ over a division ring $\Delta$,
    there is an invertible matrix 
    $R$ and a permutation matrix $\Sigma$ such that 
    \begin{align*}
        R\Phi \Sigma & = \begin{bmatrix}
            I_r & M \\
            0 & 0 
        \end{bmatrix}.
    \end{align*}
    Furthermore 
    \begin{align*}
        \Sigma \begin{bmatrix} -M\\ I_{n-r}\end{bmatrix}
    \end{align*}
    is a kernel map $\iota:\Delta^{n-r}\to \Delta^n$.
\end{proposition}
\begin{proof}
    Augment the matrix $\Phi$ by $I_m$ to create a tableaux
    $T=[I_m | \Phi]$.  
    The \emph{admissible} rows are initially $\{1,\ldots,m\}$.
    Choose an admissible row $i$.  Scan the $\Phi$ block for 
    a column $j$ with $\Phi_{ij}\neq 0$. Since we 
    are over a division ring $\Phi_{ij}$ is therefore invertible.


    Select a non-zero column $j$ in the $\Phi$ 
    block that is non-zero.  
    \begin{align*}
        \begin{bmatrix}
            \Phi_{ij}^{-1} I_n 
        \end{bmatrix}
    \end{align*}    
\end{proof}
\begin{lstlisting}[language=Sava]
def enteringCol():Fin[n]

\end{lstlisting}

Suppose 
\begin{align*}
    \Phi & = 
    \begin{bmatrix}
        1 & 0 & -2 & 1\\
        0 & 1 & 2 & 2\\
        0 & 0 & 0 & 0
    \end{bmatrix}
\end{align*}




\subsection{Kernels in Typed language}

The diagram language clarifies how kernels can be thought of as functions and 
functions that have a maximal quality.  Yet, programs do not think in pictures, 
we do.  So we need to translate the same ideas to a syntax we can turn into a 
program.  Here is how.

First the question depends on $\Omega$-modules $V$ and $W$ and a linear map $\varphi:V\to W$ 
known first from context, denoted $\mathsf{ctx}$ or $\Gamma$.  Using the notation 
$P\vdash Q$ to say ``$P$ leads to $Q$'', also denoted $\frac{P}{Q}$, then we can 
state this as forming the kernel under a list of assumed knowledge.
\begin{gather}
    \tag{$\form{\ker}$}
    \frac{
        \mathsf{ctx} \vdash V,W:{_{\Omega} \mathsf{Mod}}\qquad
        \varphi:\mathsf{Lin}_{\Omega}(V,W)
    }{
        \mathsf{ctx}\vdash \ker\varphi:\mathsf{Type}
    }
\end{gather}
The label $\form{\ker}$ stands for \emph{formation}.  Programs write this in
many different ways usually by introducing some keywords like ``import''
and ``use X from Y'.  To introduce a new type of data a keyword such as ``class''
or ``type'' is used.  For example, the following pseudo-code reflects the 
content of $(\form{\ker})$ but in a dialect similar to several modern 
procedural programming 
languages such as C++ and Java.
\begin{lstlisting}[language=Sava]
using V,W:Mod[Omega], Phi:Lin[V,W] from ctx
class Ker[Phi] {...}
\end{lstlisting}
For those using functional programming languages like OCaml or Haskell the following syntax offers 
as similar translation.
\begin{lstlisting}[language=Hidris]
import V,W:Mod Omega, Phi:Lin V W from ctx
type Ker Phi
\end{lstlisting}



\begin{lstfloat}[!htbp]
\begin{lstlisting}[language=Sava]
// Procedural style code
class Ker[Phi](k:K,kappa:K->V) where (Phi(kappa(k)) == 0)
// usage 
Phi = ...; k = ...; kappa = ...;
x = new Ker[Phi](k,kappa)
\end{lstlisting}
\begin{lstlisting}[language=Hidris]
--- Functional style code
type Ker Phi
null: (k:K)-> (kappa:K->V)-> (Phi kappa k == 0)-> Ker Phi
--- usage 
Phi= ...; k= ...; kappa= ...;
x= null Phi k kappa --- system checks Phi kappa k == 0
\end{lstlisting}    
\caption{An introduction of data to a kernel.}
\label{lst:kernel-intro}
\end{lstfloat}
    
Next the diagram above captured the high-level movement of that data without 
ever considering the actual data.  The programs will certainly need these 
data.  The premise from the diagram is that any data $k:K$ which is found to 
have $\kappa(k):V$ where $\varphi(\kappa(j))=0$ (see the diagrams above) must 
produce data in $\ker\varphi$.
Any such data $k:K$ is meant to produce data in the kernel, because the 
kernel is the largest such structure.  So we include such a rule.
\begin{gather}
    \tag{$\intro{\ker}$}
    \frac{
        k:K\qquad \kappa:K\to V\qquad pf:\varphi(\kappa(k))=_W 0
    }{
        \mathsf{null}(\kappa(k)):\ker\varphi
    }
\end{gather}
The $\intro{\ker}$ here is for \emph{introduction} because data is 
being introduced of the desired type.  
Most readers will not be prepared for the meaning of symbols like:
\[
    pf:\varphi(\kappa(k))=_W 0
\] 
Programmers however are uniquely well-positioned to guess the meaning. We want
some data $pf$ that has the type $\varphi(\kappa(k))=0$ in $W$. Said another
way, we need someone to provide a proof of that equality. In programs this can
be done by several tricks most common are what are known as \emph{guards} or
\emph{rails}.  These are a type of documentation added to a program to let the
programming language enforce that data is used in restricted ways.  In this
case, no one can introduce a term in the kernel without proof. In code this can
be captured in a number of ways, Listing~\ref{lst:kernel-intro} is one option.

% The introduction of data of some kind is known 
% to programmers as a \emph{constructor} and many languages make special 
% rules to specify constructors.  For uniformity and simplicity we here 
% use `def' to introduce all service functions to a data type and name 
% the function to match the notation used in the mathematical formalism.
% But any real program will adapt the vocabulary and style to fit with 
% conventions.


Now it is time to use data in the kernel.  It is clear how this should 
proceed, anything in the kernel can be mapped to $0$ in $W$ or to a 
value in $V$ which will map to $0$ under $\varphi$.  The rules are therefore 
as follows.    
\begin{gather}
    \tag{$\elim{\ker}$}
    \frac{
        x:\ker\phi
    }{
        0:W
    }\qquad
    \frac{
        x:\ker\phi
    }{
        \iota(x):V
    }
\end{gather}
The name $\elim{\ker}$ stands for \emph{elimination} as we are eliminating 
the kernel type to get to new types.
In code this might be done as shown in the code fragment Listing~\ref{lst:kernel-elim}.
\begin{lstfloat}[!htbp]
\begin{lstlisting}[language=Sava]
// Procedural style code
class Ker[Phi](k:K,kappa:K->V) where (Phi(kappa(k))== 0){
    def iota:V = ...
    def zero:W = ...
}
// usage 
x = new Ker[Phi](k,kappa)
v = x.iota
\end{lstlisting}
\begin{lstlisting}[language=Hidris]
--- Functional style code
iota: Ker Phi -> V
...
zero: Ker Phi -> W
...
--- usage 
x= null Phi k kappa --- system checks Phi kappa k == 0
v = iota x
\end{lstlisting}    
\caption{Using of data of kernel type.}
\label{lst:kernel-elim}
\end{lstfloat}

Finally we need to do some computing somewhere and we learn 
what to compute by inspecting the condition of ``commutative diagrams''.
\begin{gather}
    \tag{$\comp{\ker}$}
    \frac{
        k:K\qquad \kappa:K\to V\qquad pf:\varphi(\kappa(j))=_W 0
    }{
        \iota(x) \defeq \kappa(k)
    }
\end{gather}
All together this comes together in software in many different ways 
each designed around different techniques to improve how we read and 
execute code.  Listing~\ref{lst:kernel-comp} provides some of the options.
\begin{lstfloat}[!htbp]
\begin{lstlisting}[language=Sava]
// Procedural style code
class Ker[Phi](k:K,kappa:K->V) where (Phi(kappa(k))== 0){
    def iota:V = kappa(k)
    def zero:W = 0
}
// usage 
x = new Ker[Phi](k,kappa)
v = x.iota
\end{lstlisting}
\begin{lstlisting}[language=Hidris]
--- Functional style code
iota: Ker Phi -> V
iota x = kappa k where x = null k kappa
zero: Ker Phi -> W
zero x = 0
--- usage 
x= null Phi k kappa --- system checks Phi kappa k == 0
v = iota x
\end{lstlisting}    
\caption{Complete data type for kernels.}
\label{lst:kernel-comp}
\end{lstfloat}
% \begin{lstfloat}
% \begin{lstlisting}[language=Sava]
% using V,W:Mod[Omega], Phi:Lin[V,W] from ctx
% class Ker[Phi] {
%     private v:V

%     def null(j:J, kappa:J->V, 
%         require Phi(kappa(j)) == 0) {
%         v = kappa(j)    
%     }
%     def iota:V = v
%     def zero:W = 0
% }
% \end{lstlisting}
% \caption{A complete data type for kernels}    
% \label{lst:kernel-comp}
% \end{lstfloat}


\subsection*{Problems}
\begin{prob}
    Identify the type of elementary matrix displayed below.
\begin{gather*}
    A =
    \begin{bmatrix}
         1 & 0 & 0 & 0 \\
         0 & 1 & 0 & \alpha \\
         0 & 0 & 1 & 0 \\
         0 & 0 & 0 & 1 
    \end{bmatrix}
    \quad 
    B=
    \begin{bmatrix} 
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        1 & 0 & 0 
    \end{bmatrix}
    \quad 
    C=
    \begin{bmatrix}
        42 & 0 \\
        0 & \sqrt[128]{17}
    \end{bmatrix}
    \quad 
    D = 
    \begin{bmatrix}
        0 & 0 & 0 & 1 \\ 
        0 & 0 & 1 & 0 \\
        0 & 1 & 0 & 0 \\
        1 & 0 & 0 & 0
    \end{bmatrix}
\end{gather*}
\end{prob}

\begin{prob}
    Complete the reduction of the matrix 
    \begin{align*}
        \begin{bmatrix}
            4 & 12 \\
            6 & 30 \\
            27 & 24
         \end{bmatrix}
    \end{align*}    
\end{prob}

\begin{prob}
    Find access to a computer algebra system 
    and compute kernels of random $1000\times 1200$, 
    $1000 \times 1000$ and $1200\time 1000$ matrices.
\end{prob}

\begin{prob} 
    Prove that any two kernels are isomorphic.
\end{prob}


\end{document}